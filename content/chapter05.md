**贝叶斯算法**

条件概率
P(A,B)=P(A)*P(B|A)
P(A,B)表示A和B同时发生的概率
P(A)*P(B|A) 表示在A发生的概率乘以A发生的情况下B发生的概率  ----表示A和B的交集

**同理 ：**

P(A,B)=P(B)*P(A|B)
所以
P(B|A)=P(B)*P(A|B) / P(A)

在朴素贝叶斯中根据自变量推算类别 也可由下列公式
P(c|x)=P(c)*P(x|c) / P(x)

**朴素贝叶斯分类器：**

基于已知类别，每个属性互相独立。属性条件独立性假设
P(c)*P(x|c) / P(x)=P(c)/P(xi) ∏ P(xi|c)
由于P(xi)是个常量，对于每个类别来说都一样,所以
P(c) ∏ P(xi|c)

P(c)=|Dc| / |D|
P(xi|c)= |Dc,xi| / |Dc|

所以假设P(好瓜)= 8/17
P(坏瓜)= 9/17
P(青绿|好瓜)=3/8
P(青绿|坏瓜)=5/8
以上是观察样本，也就是先验概率

那么 求自变量是青绿 ，是属于好瓜还是坏瓜
P(好瓜)*P(青绿|好瓜)=3/17
P(坏瓜)*P(青绿|坏瓜)=5/17
所以是属于坏瓜的概率大
自变量多 就都乘起来

如果自变量属性是连续值
那么则使用概率密度函数来求概率
概率密度函数公式一般是正态分布，公式上网查


**似然函数**

似然（likelihood）这个词其实和概率（probability）是差不多的意思，Colins字典这么解释：The likelihood of something happening is how likely it is to happen. 你把likelihood换成probability，这解释也读得通。但是在统计里面，似然函数和概率函数却是两个不同的概念（其实也很相近就是了）。

对于这个函数：

P(x|θ)
输入有两个：x表示某一个具体的数据；θ表示模型的参数。

如果θ是已知确定的，x是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点x，其出现概率是多少。

如果x是已知确定的，θ是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现x这个样本点的概率是多少。
这有点像“一菜两吃”的意思。其实这样的形式我们以前也不是没遇到过。例如，f(x,y)=xY, 即x的y次方。
如果x是已知确定的(例如x=2)，这就是f(y)=2Y, 这是指数函数。 如果y是已知确定的(例如y=2)，这就是f(x)=x2，这是二次函数。同一个数学形式，从不同的变量角度观察，可以有不同的名字。

**极大似然估计**

假设每个样本都是独立同分布的
P(X|θ)=∏ (xi|θ)
根据数据采样来估计概率分布参数的方法
对θ进行极大似然估计，找到最大化似然函数P(X|θ)的参数θ，找到这个参数使这些数据出现的可能性最大

因为连乘容易造成下溢，所以取对数相乘。